##################################################################################
#                  INSTALAÇÃO E CARREGAMENTO DE PACOTES NECESSÁRIOS             #
##################################################################################
#Pacotes utilizados
pacotes <- c("tidytext","ggplot2","dplyr","tibble","gutenbergr","wordcloud","stringr","SnowballC","widyr","janeaustenr")
if(sum(as.numeric(!pacotes %in% installed.packages())) != 0){
instalador <- pacotes[!pacotes %in% installed.packages()]
for(i in 1:length(instalador)) {
install.packages(instalador, dependencies = T)
break()}
sapply(pacotes, require, character = T)
} else {
sapply(pacotes, require, character = T)
}
textos <- gutenberg_metadata
View(textos)
#Vamos usar perter pan e Moby Dick
livros <- gutenberg_download(c(15,16))
View(livros)
st <- starwars
View(st)
install.packages("tidyverse")
library("tidyverse")
library("dplyr")
library("tidytext")
library("wordcloud")
library("stringr")
library("SnowballC")
library(readr)
library("readr")
?read_tsv
text <- read_tsv(Vianet.txt)
text <- read_tsv("Vianet.txt")
View(text)
View(livros)
instagram_comments <- read_tsv("Vianet.txt")
#Vamos retirar números - pode ser qualquer coisa
nums <- instagram_comments %>% filter(str_detect(text, "^[0-9]")) %>% select(text)
View(livros)
instagram_comments <- read_tsv("Vianet.txt", show_col_types = FALSE)
#Vamos nos livrar das stop words e obter os tokens
instagram_comments <- instagram_comments %>%  unnest_tokens(word, text) %>%  anti_join(stop_words)
?unnest_tokens
install.packages("stopwords")
View(instagram_comments)
Encoding(instagram_comments) <- "ASCII"
Encoding(instagram_comments$Brin) <- "ASCII"
#vamos retirar os acentos
for (i in 1:nrow(instagram_comments))
{
instagram_comments$Brin[i] <- iconv(instagram_comments$Brin[i], to = "ASCII//TRANSLIT")
}
#Unnest tokens
instagram_comments <- instagram_comments %>%  unnest_tokens(word, text)
#Unnest tokens
instagram_comments <- instagram_comments$Brin %>%  unnest_tokens(word, text)
#Unnest tokens
instagram_comments <- instagram_comments %>%  unnest_tokens(word, Brin)
View(instagram_comments)
#Excluímos stop words em português - instalar stop words
instagram_comments_nostop <- instagram_comments %>% anti_join(get_stopwords(language = 'pt'))
View(instagram_comments_nostop)
#Word cloud - teste word e stem
comments_count <- instagram_comments_nostop %>% select(word) %>% count(word, sort = TRUE)
pal <- brewer.pal(8,"Dark2")
comments_count %>% with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
comments_count %>% with(wordcloud(word, n, random.order = FALSE, max.words = 20, colors=pal))
#excluindo garbage
no_stop=instagram_comments_nostop[!grepl("picture",instagram_comments_nostop$word),]
no_stop=instagram_comments_nostop[!grepl("1dreply",instagram_comments_nostop$word),]
no_stop=instagram_comments_nostop[!grepl("1dreply",instagram_comments_nostop$word),]
View(no_stop)
#excluindo garbage
no_stop=instagram_comments_nostop[!grepl("picture",instagram_comments_nostop$word),]
View(no_stop)
no_stop=nostop[!grepl("1dreply",nostop$word),]
no_stop=nostop[!grepl("1dreply",no_stop$word),]
no_stop=no_stop[!grepl("1dreply",no_stop$word),]
no_stop=no_stop[!grepl("profile",no_stop$word),]
#Word cloud - teste word e stem
comments_count <- nostop %>% select(word) %>% count(word, sort = TRUE)
#Word cloud - teste word e stem
comments_count <- no_stop %>% select(word) %>% count(word, sort = TRUE)
pal <- brewer.pal(8,"Dark2")
comments_count %>% with(wordcloud(word, n, random.order = FALSE, max.words = 20, colors=pal))
comments_count %>% with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
pal <- brewer.pal(8,"Dark2")
comments_count %>% with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
comments_count %>% with(wordcloud(word, n, random.order = FALSE, max.words = 20, colors=pal))
View(no_stop)
clear
clean
cls
library("readr")
library("dplyr")
library("tidytext")
library("wordcloud")
library("stringr")
library("SnowballC")
install.packages("stopwords")
install.packages("stopwords")
instagram_comments <- read_tsv("Vianet.txt", show_col_types = FALSE)
View(instagram_comments)
Encoding(instagram_comments$Brin) <- "ASCII"
View(instagram_comments)
instagram_comments <- read_tsv("Vianet.txt", show_col_types = FALSE)
View(instagram_comments)
#vamos retirar os acentos
for (i in 1:nrow(instagram_comments))
{
instagram_comments$Brin[i] <- iconv(instagram_comments$Brin[i], to = "ASCII//TRANSLIT")
}
View(instagram_comments)
instagram_comments <- read_tsv("Vianet.txt", show_col_types = FALSE)
Encoding(instagram_comments$Brin) <- "ASCII"
#vamos retirar os acentos
for (i in 1:nrow(instagram_comments))
{
instagram_comments$Brin[i] <- iconv(instagram_comments$Brin[i], to = "ASCII//TRANSLIT")
}
View(instagram_comments)
instagram_comments <- read_tsv("Vianet.txt", show_col_types = FALSE)
View(instagram_comments)
#Unnest tokens
instagram_comments <- instagram_comments %>%  unnest_tokens(word, Brin)
View(instagram_comments)
#Excluímos stop words em português - instalar stop words
instagram_comments_nostop <- instagram_comments %>% anti_join(get_stopwords(language = 'pt'))
View(instagram_comments_nostop)
#excluindo garbage
no_stop=instagram_comments_nostop[!grepl("picture",instagram_comments_nostop$word),]
no_stop=no_stop[!grepl("1dreply",no_stop$word),]
no_stop=no_stop[!grepl("profile",no_stop$word),]
#Word cloud - teste word e stem
comments_count <- no_stop %>% select(word) %>% count(word, sort = TRUE)
pal <- brewer.pal(8,"Dark2")
comments_count %>% with(wordcloud(word, n, random.order = FALSE, max.words = 20, colors=pal))
pal <- brewer.pal(8,"Dark2")
comments_count %>% with(wordcloud(word, n, random.order = FALSE, max.words = 30, colors=pal))
comments_count %>% with(wordcloud(word, n, random.order = FALSE, max.words = 30, colors=pal))
no_stop=no_stop[!grepl("3",no_stop$word),]
#Word cloud - teste word e stem
comments_count <- no_stop %>% select(word) %>% count(word, sort = TRUE)
pal <- brewer.pal(8,"Dark2")
comments_count %>% with(wordcloud(word, n, random.order = FALSE, max.words = 30, colors=pal))
comments_count %>% with(wordcloud(word, n, random.order = FALSE, max.words = 40, colors=pal))
comments_count %>% with(wordcloud(word, n, random.order = FALSE, max.words = 20, colors=pal))
comments_count %>% with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
comments_count %>% with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors=pal))
View(instagram_comments_nostop)
View(no_stop)
##################################################################################
#                  INSTALAÇÃO E CARREGAMENTO DE PACOTES NECESSÁRIOS             #
##################################################################################
#Pacotes utilizados
pacotes <- c("tidytext","ggplot2","dplyr","tibble","gutenbergr","wordcloud","stringr","SnowballC","widyr","janeaustenr")
if(sum(as.numeric(!pacotes %in% installed.packages())) != 0){
instalador <- pacotes[!pacotes %in% installed.packages()]
for(i in 1:length(instalador)) {
install.packages(instalador, dependencies = T)
break()}
sapply(pacotes, require, character = T)
} else {
sapply(pacotes, require, character = T)
}
library("readr")
library("dplyr")
library("tidytext")
library("wordcloud")
library("stringr")
library("SnowballC")
install.packages("stopwords")
word_card <- read_tsv("word_card.txt", show_col_types = FALSE)
#Word cloud - teste word e stem
word_count <- word_card %>% select(WORDS) %>% count(WORDS, sort = TRUE)
pal <- brewer.pal(8,"Dark2")
word_count %>% with(wordcloud(WORDS, n, random.order = FALSE, max.words = 50, colors=pal))
pal <- brewer.pal(10,"Dark2")
word_count %>% with(wordcloud(WORDS, n, random.order = FALSE, max.words = 50, colors=pal))
pal <- brewer.pal(10,"Dark2")
word_count %>% with(wordcloud(WORDS, n, random.order = FALSE, max.words = 50, colors=pal))
pal <- brewer.pal(7,"Dark2")
word_count %>% with(wordcloud(WORDS, n, random.order = TRUE, max.words = 50, colors=pal))
pal <- brewer.pal(6,"Dark2")
word_count %>% with(wordcloud(WORDS, n, random.order = TRUE, max.words = 50, colors=pal))
pal <- brewer.pal(6,"Dark2")
word_count %>% with(wordcloud(WORDS, n, random.order = TRUE, max.words = 50, colors=pal))
pal <- brewer.pal(5,"Dark2")
word_count %>% with(wordcloud(WORDS, n, random.order = TRUE, max.words = 50, colors=pal))
pal <- brewer.pal(5,"Dark2")
word_count %>% with(wordcloud(WORDS, n, random.order = TRUE, max.words = 50, colors=pal))
pal <- brewer.pal(5,"Dark2")
word_count %>% with(wordcloud(WORDS, n, random.order = TRUE, max.words = 10, colors=pal))
word_count %>% with(wordcloud(WORDS, n, random.order = TRUE, max.words = 14, colors=pal))
word_count %>% with(wordcloud(WORDS, n, random.order = TRUE, max.words = 14, colors=pal))
word_count %>% with(wordcloud(WORDS, n, random.order = TRUE, max.words = 14, colors=pal))
word_count %>% with(wordcloud(WORDS, n, random.order = TRUE, max.words = 14, colors=pal))
word_count %>% with(wordcloud(WORDS, n, random.order = TRUE, max.words = 14, colors=pal))
word_count %>% with(wordcloud(WORDS, n, random.order = TRUE, max.words = 14, colors=pal))
word_count %>% with(wordcloud(WORDS, n, random.order = TRUE, max.words = 100, colors=pal))
?wordcloud
word_count %>% with(wordcloud(WORDS, n, scale = 10, random.order = TRUE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(10), random.order = TRUE, max.words = 100, colors=pal))
?wordcloud
word_count %>% with(wordcloud(WORDS, n, scale = c(10,10), random.order = TRUE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(10,2), random.order = TRUE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(10,1), random.order = TRUE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(10,3), random.order = TRUE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(10,3), random.order = TRUE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(4,3), random.order = TRUE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(4,3), random.order = TRUE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,3), random.order = TRUE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,3), random.order = TRUE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,3), random.order = FALSE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,3), random.order = FALSE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(3,2), random.order = FALSE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(3,2), random.order = FALSE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(1,3), random.order = FALSE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,3), random.order = FALSE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(3,3), random.order = FALSE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, random.order = FALSE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,2), random.order = FALSE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,2), random.order = FALSE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(4,2), random.order = FALSE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(3,2), random.order = FALSE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(3,2), random.order = FALSE, max.words = 100, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(3,2), random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,3), random.order = FALSE, max.words = 50, colors=pal))
View(word_count)
word_count %>% with(wordcloud(WORDS, n, scale = c(3,4), random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(1,2), random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(1,2), random.order = FALSE, max.words = 14, colors=pal))
word_card <- read_tsv("word_card.txt", show_col_types = FALSE)
#Word cloud - teste word e stem
word_count <- word_card %>% select(WORDS) %>% count(WORDS, sort = TRUE)
pal <- brewer.pal(5,"Dark2")
word_count %>% with(wordcloud(WORDS, n, scale = c(1,2), random.order = FALSE, max.words = 14, colors=pal))
word_card <- read_tsv("word_card.txt", show_col_types = FALSE)
#Word cloud - teste word e stem
word_count <- word_card %>% select(WORDS) %>% count(WORDS, sort = TRUE)
pal <- brewer.pal(5,"Dark2")
word_count %>% with(wordcloud(WORDS, n, scale = c(1,2), random.order = FALSE, max.words = 14, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,3), random.order = FALSE, max.words = 14, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,3), random.order = FALSE, max.words = 14, colors=pal))
word_count %>% with(wordcloud(WORDS, n, random.order = FALSE, max.words = 14, colors=pal))
word_count %>% with(wordcloud(WORDS, n, random.order = FALSE, max.words = 14, colors=pal))
word_count %>% with(wordcloud(WORDS, n, random.order = FALSE, max.words = 50, colors=pal))
pal <- brewer.pal(4,"Dark2")
word_count %>% with(wordcloud(WORDS, n, random.order = FALSE, max.words = 50, colors=pal))
pal <- brewer.pal(3,"Dark2")
word_count %>% with(wordcloud(WORDS, n, random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(5.6),random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(5,6),random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,6),random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,5),random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,4),random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,3),random.order = FALSE, max.words = 50, colors=pal))
pal <- brewer.pal(8,"Dark2")
word_count %>% with(wordcloud(WORDS, n, scale = c(2,3),random.order = FALSE, max.words = 50, colors=pal))
?wordcloud
word_count %>% with(wordcloud(WORDS, n, scale = c(2,.3),random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,.2),random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,.2),random.order = FALSE, max.words = 50, colors=pal))
library("readr")
library("dplyr")
library("tidytext")
library("wordcloud")
library("stringr")
library("SnowballC")
?brewer.pal
word_card <- read_tsv("LinkedIn.txt", show_col_types = FALSE)
#Word cloud - teste word e stem
word_count <- word_card %>% select(WORDS) %>% count(WORDS, sort = TRUE)
pal <- brewer.pal(5,"Greys")
word_count %>% with(wordcloud(WORDS, n, scale = c(3,.2),random.order = FALSE, max.words = 50, colors=pal))
word_card <- read_tsv("LinkedIn.txt", show_col_types = FALSE)
#Word cloud - teste word e stem
word_count <- word_card %>% select(WORDS) %>% count(WORDS, sort = TRUE)
pal <- brewer.pal(5,"Greys")
word_count %>% with(wordcloud(WORDS, n, scale = c(3,.2),random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(3,.2),random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(3,.2),random.order = FALSE, max.words = 50, colors=pal))
word_card <- read_tsv("LinkedIn.txt", show_col_types = FALSE)
#Word cloud - teste word e stem
word_count <- word_card %>% select(WORDS) %>% count(WORDS, sort = TRUE)
pal <- brewer.pal(5,"Greys")
word_count %>% with(wordcloud(WORDS, n, scale = c(3,.2),random.order = FALSE, max.words = 50, colors=pal))
word_card <- read_tsv("LinkedIn.txt", show_col_types = FALSE)
#Word cloud - teste word e stem
word_count <- word_card %>% select(WORDS) %>% count(WORDS, sort = TRUE)
pal <- brewer.pal(5,"Greys")
word_count %>% with(wordcloud(WORDS, n, scale = c(3,.2),random.order = FALSE, max.words = 50, colors=pal))
library("readr")
library("dplyr")
library("tidytext")
library("wordcloud")
library("stringr")
library("SnowballC")
word_card <- read_tsv("LinkedIn.txt", show_col_types = FALSE)
View(word_card)
word_card <- read_tsv("LinkedIn.txt", show_col_types = FALSE)
#Word cloud - teste word e stem
word_count <- word_card %>% select(WORDS) %>% count(WORDS, sort = TRUE)
View(word_count)
pal <- brewer.pal(5,"Greys")
word_count %>% with(wordcloud(WORDS, n, scale = c(3,.2),random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(4,.2),random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,.2),random.order = FALSE, max.words = 50, colors=pal))
pal <- brewer.pal(6,"Greys")
word_count %>% with(wordcloud(WORDS, n, scale = c(2,.2),random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,.2),random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,.2),random.order = FALSE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,.2),random.order = TRUE, max.words = 50, colors=pal))
word_count %>% with(wordcloud(WORDS, n, scale = c(2,.2),random.order = TRUE, max.words = 50, colors=pal))
# import csv file
myExploratoryData <- read.csv("~/Python/Exercise_files/02_02/exploratory-r.csv")
View(myExploratoryData)
# get a quick snapshot of your data
head(myExploratoryData)
hist(myExploratoryData$cpa)
# shift the names to each row
row.names(myExploratoryData) <- myExploratoryData$keyword
View(myExploratoryData)
# review that transformation
head(myExploratoryData)
# transform into a matrix
myDataMatrix <- data.matrix(myExploratoryData)
View(myDataMatrix)
# generate our heatmap
heatmap(myDataMatrix, Row = NA, Colv = NA, scale = "column")
# generate our heatmap
heatmap(myDataMatrix, Rowv = NA, Colv = NA, scale = "column")
# Connect to our data
myRegressionData <- read.csv("~/Desktop/Exercise_Files/03_02/regression-r.csv")
# Connect to our data
myRegressionData <- read.csv("~/Área de Trabalho/Exercise_Files/03_02/regression-r.csv")
# Connect to our data
myRegressionData <- read.csv("G:/Área de Trabalho/Exercise_Files/03_02/regression-r.csv")
# Connect to our data
myRegressionData <- read.csv("G:/Desktop/Exercise_Files/03_02/regression-r.csv")
# Connect to our data
myRegressionData <- read.csv("C:/Área de Trabalho/Exercise_Files/03_02/regression-r.csv")
# Connect to our data
myRegressionData <- read.csv("C:/Users/clate/Desktop/Exercise_Files/03_02/regression-r.csv")
View(myRegressionData)
# Plot our data (broadcast & sales)
plot(myRegressionData$BROADCAST, myRegressionData$NET.SALES)
# Fit a line
myLm <- lm(myRegressionData$NET.SALES - myRegressionData$BROADCAST)
# Fit a line
myLmV <- myRegressionData$NET.SALES - myRegressionData$BROADCAST
myLm <- lm(myLmV)
# Fit a line
myLn <- lm(myRegressionData$NET.SALES - myRegressionData$BROADCAST)
# Visualize the line
lines(myRegressionData$BROADCAST, myLn$fitted)
# Visualize the line
lines(myRegressionData$BROADCAST, myLm$fitted)
# Fit a line
myLm <- lm(myRegressionData$NET.SALES - myRegressionData$BROADCAST)
?lm
View(myRegressionData)
# Fit a line
myLm <- lm(myRegressionData$NET.SALES ~ myRegressionData$BROADCAST)
# Visualize the line
lines(myRegressionData$BROADCAST, myLm$fitted)
# Show our coefficients
myLm$coefficients
